# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ynS6sYItPu46QuZesZoxJ_kYIVxIpKQE
"""

import numpy as np
from scipy.stats import norm, poisson

class CovidMDP:
    def __init__(self, num_beds=650, population=[0.38, 0.33, 0.23, 0.06], num_weeks=4,
                 bed_conversion_cost=5000, normal_bed_denial_cost=1000, covid_bed_denial_cost=5000):
        self.num_beds = num_beds
        self.population = np.array(population)
        self.num_weeks = num_weeks
        self.bed_conversion_cost = bed_conversion_cost
        self.normal_bed_denial_cost = normal_bed_denial_cost
        self.covid_bed_denial_cost = covid_bed_denial_cost
        self.start_state = (self.num_beds, np.zeros(len(self.population)), np.zeros(len(self.population)),
                            np.zeros(len(self.population)), np.zeros(len(self.population)))
        self.end_state = None
        self.num_states = 0
        self.num_actions = 0
        self.states = {}
        self.actions = {}
        self.rewards = {}
        self.transition_probs = {}

        # Define the possible states and actions
        for i in range(self.num_beds + 1):
            for j in range(self.num_beds + 1 - i):
                for k in range(self.num_weeks):
                    for l in range(len(self.population)):
                        for m in range(len(self.population)):
                            state = (i, j, k, l, m)
                            self.states[state] = self.num_states
                            self.num_states += 1
        for i in range(self.num_beds + 1):
            for j in range(self.num_weeks):
                action = (i, j)
                self.actions[action] = self.num_actions
                self.num_actions += 1

        # Define the reward function
        for state in self.states:
            for action in self.actions:
                r = 0
                beds_normal, beds_covid, week, pop_before, pop_after = state
                beds_normal_new = beds_normal - action[0]
                beds_covid_new = beds_covid + action[0] + action[1]
                pop_migrated = np.round(norm.rvs(0, 1, size=len(self.population)) * 1000)
                pop_before_new = pop_before + pop_migrated
                pop_after_new = pop_after + pop_migrated
                for p in range(len(self.population)):
                    requests_normal = poisson.rvs(pop_after_new[p] * beds_normal_new / self.num_beds)
                    discharges_normal = poisson.rvs(beds_normal_new)
                    denied_normal = max(requests_normal - discharges_normal, 0)
                    requests_covid = poisson.rvs(pop_after_new[p] * beds_covid_new / self.num_beds)
                    discharges_covid = poisson.rvs(beds_covid_new)
                    denied_covid = max(requests_covid - discharges_covid, 0)
                    r += self.normal_bed_denial_cost * denied_normal + \
                         self.covid_bed_denial_cost * denied_covid + \
                         self.bed_conversion_cost * (action[0] + action[1])
                self.rewards[(self.states[state], self.actions[action])] = -r

        # Define the transition probabilities
        for state in self.states:
            for action in self.actions:
                prob_dict = {}
                beds_normal, beds_covid, week, pop_before, pop_after = state
                beds_normal_new = beds_normal - action[0]
                beds_covid_new = beds_covid + action

import numpy as np

def policy_evaluation_gbike(policy, Lamda, lamda, r, t, gam):
# Implement policy evaluation for gbike problem
# Returns state value V for the given policy
    n_states = (21)**2
    n_actions = 11
    T = np.zeros((n_states, n_states, n_actions))
    R = np.zeros((n_states, n_actions))
    P = np.zeros((n_states, n_actions, n_states))
    V = np.zeros((21, 21))

    # Calculate transition probabilities and rewards for all states and actions
    for i in range(21):
        for j in range(21):
            state = i*21 + j
            for action in range(n_actions):
                na1 = min(i+action-5, 20) # number of available bikes at location 1 after action
                na2 = min(j-action+5, 20) # number of available bikes at location 2 after action
                for r1 in range(21):
                    for r2 in range(21):
                        p = poisson_prob(r1, Lamda[0]) * poisson_prob(r2, Lamda[1]) # probability of rental requests
                        rent1 = min(na1, r1) # number of bikes rented at location 1
                        rent2 = min(na2, r2) # number of bikes rented at location 2
                        reward = (rent1 + rent2) * r # rental reward
                        nr1 = na1 - rent1 + min(lamda[0], 20-na1+rent1) # number of available bikes at location 1 after returns
                        nr2 = na2 - rent2 + min(lamda[1], 20-na2+rent2) # number of available bikes at location 2 after returns
                        ns = nr1*21 + nr2 # next state
                        R[state, action] += p * reward # reward for current state and action
                        T[state, ns, action] += p # transition probability for current state, action, and next state

    # Perform policy evaluation
    while True:
        delta = 0
        for i in range(21):
            for j in range(21):
                state = i*21 + j
                v = 0
                for action in range(n_actions):
                    v_a = 0
                    for ns in range(n_states):
                        v_a += T[state, ns, action] * (R[state, action] + gam * V[ns//21, ns%21])
                    v += policy[i, j, action] * v_a
                delta = max(delta, abs(v - V[i, j]))
                V[i, j] = v
        if delta < 1e-4:
            break
    return V

def policy_improvement_gbike(V, policy, Lamda, lamda, r, t, gam):
# Implement policy improvement for gbike problem
# Returns new policy and a boolean indicating if the policy has stabilized

    n_states = (21)**2
    n_actions = 11
    new_policy = np.zeros((21, 21, n_actions))
    P = np.zeros((n_states, n_actions, n_states))
    R = np.zeros((n_states, n_actions))
    policystable = True
    for i in range(21):
        for j in range(21):
            state = i*21 + j
            for action in range(n_actions):
                na1 = min(i+action-5, 20) # number of available bikes at location 1 after action
                na2 = min(j-action+5, 20) # number of available bikes at location 2 after action
                for r1 in range(21):
                    for r2 in range(21):
                        p = poisson_prob(r1, Lamda[0]) * poisson_prob(r2, Lamda[1]) # probability of rental requests
                        rent1 = min(na1, r1) # number of bikes rented at location 1
                        rent2 = min(na2, r2) # number of bikes rented at location 2
                        reward = (rent1 + rent2) * r # rental reward
                        nr1 = na1 - rent1 + min(lamda[0], 20-na1+rent1) # number of available bikes at location 1 after returns
                        nr2 = na2 - rent2 + min(lamda[1], 20-na2+rent2) # number of available bikes at location 2 after returns
                        ns = nr1*21 + nr2 # next state
                        R[state, action] += p * reward # reward for current state and action
                        P[state, action, ns] += p # transition probability for current state, action, and next state

    # Perform policy improvement
    for i in range(21):
        for j in range(21):
            state = i*21 + j
            q = np.zeros(n_actions)
            for action in range(n_actions):
                for ns in range(n_states):
                    q[action] += P[state, action, ns] * (R[state, action] + gam * V[ns//21, ns%21])
            best_action = np.argmax(q)
            new_policy[i, j, best_action] = 1
            if not np.array_equal(new_policy[i, j, :], policy[i, j, :]):
                policystable = False
    return new_policy, policystable

def poisson_prob(n, lam):
# Calculate the probability of n occurrences given the rate parameter lam using Poisson distribution
    return lam**n * np.exp(-lam) / np.math.factorial(n)